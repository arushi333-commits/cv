{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arushi333-commits/cv/blob/main/Final_Real_vs_Fake_Face_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "epJCeJzLGRRM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epJCeJzLGRRM",
        "outputId": "b45b8d31-31d6-4b16-c98b-cf7597a3dda5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.116.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.35.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading pyngrok-7.2.12-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.12\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries for building the API and exposing it publicly\n",
        "# fastapi: A modern, fast (high-performance) web framework for building APIs.\n",
        "# uvicorn: An ASGI server implementation, used to run the FastAPI application.\n",
        "# nest-asyncio: Allows asyncio to be used in environments with an existing event loop (like Jupyter/Colab).\n",
        "# pyngrok: A Python wrapper for ngrok, which creates a secure tunnel to expose a local server to the internet.\n",
        "!pip install fastapi uvicorn nest-asyncio pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd0d0db9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "fd0d0db9",
        "outputId": "9395ebc8-f2a3-4772-b55b-663ac6bb3dc9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-70f43fc9-5c48-4c5f-9ad4-6af507532811\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-70f43fc9-5c48-4c5f-9ad4-6af507532811\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# Install the Kaggle library to interact with the Kaggle API\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Import the 'files' module from google.colab to handle file uploads in the Colab environment\n",
        "from google.colab import files\n",
        "\n",
        "# Prompt the user to upload their 'kaggle.json' API token file.\n",
        "# This file is required to authenticate with the Kaggle API.\n",
        "files.upload()\n",
        "\n",
        "# Create a directory named '.kaggle' in the user's home directory.\n",
        "# The Kaggle library expects the API token to be in this specific location.\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Copy the uploaded 'kaggle.json' file to the newly created '.kaggle' directory.\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# Set the permissions of the 'kaggle.json' file to 600 (read and write for the owner only).\n",
        "# This is a security measure to protect your API credentials.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2faf26cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2faf26cc",
        "outputId": "76725338-3884-49c7-980e-fa1e937caaae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces\n",
            "License(s): other\n",
            "Downloading 140k-real-and-fake-faces.zip to /content\n",
            "100% 3.73G/3.75G [00:41<00:00, 187MB/s]\n",
            "100% 3.75G/3.75G [00:41<00:00, 97.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the \"140k Real and Fake Faces\" dataset from Kaggle using the Kaggle API.\n",
        "# The '-d' flag specifies the dataset to download.\n",
        "!kaggle datasets download -d xhlulu/140k-real-and-fake-faces\n",
        "\n",
        "# Unzip the downloaded dataset file quietly (-q) and extract its contents into a 'data/' directory.\n",
        "!unzip -q 140k-real-and-fake-faces.zip -d data/\n",
        "\n",
        "# The unzipped files are in a nested directory structure. This command moves all files\n",
        "# from 'data/real_vs_fake/real-vs-fake/' up to the 'data/' directory for easier access.\n",
        "!mv data/real_vs_fake/real-vs-fake/* data/\n",
        "\n",
        "# Remove the now-empty 'real_vs_fake' directory to clean up the folder structure.\n",
        "!rm -rf data/real_vs_fake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9Ec-JBQg8Whi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ec-JBQg8Whi",
        "outputId": "536365f6-83d9-4c55-8599-28c82d9f5756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Downloading and saving 100 fake images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:27<00:00,  3.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Total images now in:\n",
            "   /content/data/train/fake: 50087\n",
            "   /content/data/valid/fake: 10013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import random\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "# Set the number of new fake images to download and add to the dataset.\n",
        "NUM_IMAGES = 100\n",
        "# Define the proportion of the newly downloaded images that will be used for training (80%).\n",
        "TRAIN_SPLIT = 0.8\n",
        "# Specify the directory to save the new fake training images.\n",
        "TRAIN_DIR = \"/content/data/train/fake\"\n",
        "# Specify the directory to save the new fake validation images.\n",
        "VAL_DIR = \"/content/data/valid/fake\"\n",
        "\n",
        "# --- Folder Creation ---\n",
        "# Create the training and validation directories for the fake images if they don't already exist.\n",
        "# 'exist_ok=True' prevents an error from being raised if the directories are already there.\n",
        "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
        "os.makedirs(VAL_DIR, exist_ok=True)\n",
        "\n",
        "# --- Image Download Function ---\n",
        "def download_and_save(index):\n",
        "    \"\"\"\n",
        "    Downloads a single fake face image from thispersondoesnotexist.com and saves it\n",
        "    to either the training or validation directory.\n",
        "    \"\"\"\n",
        "    url = \"https://thispersondoesnotexist.com\"\n",
        "    # Set a User-Agent header to mimic a web browser, which can help avoid being blocked.\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    try:\n",
        "        # Send an HTTP GET request to the URL with a 10-second timeout.\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        # Check if the request was successful (HTTP status code 200).\n",
        "        if response.status_code == 200:\n",
        "            # Open the image from the response content in memory and convert it to RGB format.\n",
        "            img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "            # Create a unique filename for the new image.\n",
        "            filename = f\"new_fake_{index}.jpg\"\n",
        "            # Randomly decide whether to save the image to the training or validation set\n",
        "            # based on the TRAIN_SPLIT ratio.\n",
        "            folder = TRAIN_DIR if random.random() < TRAIN_SPLIT else VAL_DIR\n",
        "            # Save the image to the selected folder.\n",
        "            img.save(os.path.join(folder, filename))\n",
        "    except Exception as e:\n",
        "        # If any error occurs during download or saving, print an error message.\n",
        "        print(f\"❌ Failed to download image {index}: {e}\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "print(f\"📥 Downloading and saving {NUM_IMAGES} fake images...\")\n",
        "# Use tqdm to create a progress bar for the download loop.\n",
        "for i in tqdm(range(NUM_IMAGES)):\n",
        "    download_and_save(i)\n",
        "\n",
        "# --- Summary ---\n",
        "# Print a summary of the total number of images now present in the training and validation directories.\n",
        "print(f\"✅ Total images now in:\")\n",
        "print(f\"   {TRAIN_DIR}: {len(os.listdir(TRAIN_DIR))}\")\n",
        "print(f\"   {VAL_DIR}: {len(os.listdir(VAL_DIR))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20-6SYE51c4l",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20-6SYE51c4l",
        "outputId": "9f8fea15-1dd4-4bec-850c-0f416984f117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Class to index mapping: {'fake': 0, 'real': 1}\n",
            "✅ Train size: 100087\n",
            "✅ Valid size: 20013\n",
            "✅ Test size: 20000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- Configuration ---\n",
        "# Path to the root data directory, which contains 'train', 'valid', and 'test' subdirectories.\n",
        "data_dir = \"/content/data\"\n",
        "\n",
        "# --- Image Transformations ---\n",
        "# Define a sequence of transformations to be applied to each image as it is loaded.\n",
        "transform = transforms.Compose([\n",
        "    # Resize every image to a fixed size of 128x128 pixels.\n",
        "    # This ensures that all images in a batch have the same dimensions.\n",
        "    transforms.Resize((128, 128)),\n",
        "    # Convert the image from a PIL Image format to a PyTorch Tensor.\n",
        "    # This also scales the pixel values from the range [0, 255] to [0.0, 1.0].\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# --- Dataset Loading ---\n",
        "# Load the training, validation, and test datasets using torchvision's ImageFolder.\n",
        "# ImageFolder automatically finds images in subdirectories and assigns labels based on the folder names.\n",
        "train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform)\n",
        "valid_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'valid'), transform=transform)\n",
        "test_dataset  = datasets.ImageFolder(root=os.path.join(data_dir, 'test'),  transform=transform)\n",
        "\n",
        "# --- DataLoader Creation ---\n",
        "# Create DataLoader objects for each dataset to efficiently load data in batches.\n",
        "# batch_size=64: Load 64 images at a time.\n",
        "# shuffle=True (for training): Randomly shuffle the training data at the beginning of each epoch\n",
        "# to prevent the model from learning the order of the data.\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=64, shuffle=False)\n",
        "\n",
        "# --- Sanity Check ---\n",
        "# Print out some information to verify that the data has been loaded correctly.\n",
        "print(\"✅ Class to index mapping:\", train_dataset.class_to_idx)\n",
        "print(\"✅ Train size:\", len(train_dataset))\n",
        "print(\"✅ Valid size:\", len(valid_dataset))\n",
        "print(\"✅ Test size:\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "myQFoMxL1ozZ",
      "metadata": {
        "id": "myQFoMxL1ozZ"
      },
      "outputs": [],
      "source": [
        "# Define the CNN architecture for classifying real vs fake faces\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FaceClassifierCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FaceClassifierCNN, self).__init__()\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
        "         # Max pooling layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.fc1 = nn.Linear(64 * 30 * 30, 128)\n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # [64, 32, 63, 63] , Max pooling layer, First convolutional layer\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # [64, 64, 30, 30], Max pooling layer , Second convolutional layer\n",
        "        x = x.view(-1, 64 * 30 * 30)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GXYpHKUO3KKt",
      "metadata": {
        "id": "GXYpHKUO3KKt"
      },
      "outputs": [],
      "source": [
        "# Set model to evaluation mode to disable dropout/batchnorm updates\n",
        "def train_model(model, train_loader, valid_loader, epochs=5, device='cuda'):\n",
        "    \"\"\"\n",
        "    Trains and validates a PyTorch model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to train.\n",
        "        train_loader (torch.utils.data.DataLoader): DataLoader for the training set.\n",
        "        valid_loader (torch.utils.data.DataLoader): DataLoader for the validation set.\n",
        "        epochs (int): The number of training epochs.\n",
        "        device (str): The device to train on ('cuda' or 'cpu').\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    # Move the model to the specified device\n",
        "    model.to(device)\n",
        "\n",
        "    # Loop through each epoch\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"🔁 Starting epoch {epoch+1}\")\n",
        "        # Set the model to training mode\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Loop through the training data\n",
        "        for images, labels in train_loader:\n",
        "            # Move images and labels to the specified device\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            # Calculate the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the running loss and accuracy\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate the training accuracy\n",
        "        train_acc = 100 * correct / total\n",
        "\n",
        "        # --- Validation ---\n",
        "        # Set the model to evaluation mode\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        # Disable gradient calculations\n",
        "        with torch.no_grad():\n",
        "            # Loop through the validation data\n",
        "            for images, labels in valid_loader:\n",
        "                # Move images and labels to the specified device\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                # Forward pass\n",
        "                outputs = model(images)\n",
        "                # Get the predicted labels\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                # Update the total and correct counts\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate the validation accuracy\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "\n",
        "        # Print the epoch results\n",
        "        print(f\"📊 Epoch {epoch+1}/{epochs}, Loss: {running_loss:.4f}, \"\n",
        "              f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ur6DLaLG6ZIt",
      "metadata": {
        "id": "ur6DLaLG6ZIt"
      },
      "outputs": [],
      "source": [
        "# Optional: Create smaller subsets of the datasets for faster testing.\n",
        "# This is useful for quickly iterating and debugging the model without waiting for the full dataset to train.\n",
        "from torch.utils.data import Subset\n",
        "import random\n",
        "\n",
        "# Create a subset of the training dataset with 5000 random samples\n",
        "small_train_dataset = Subset(train_dataset, random.sample(range(len(train_dataset)), 5000))\n",
        "# Create a subset of the validation dataset with 1000 random samples\n",
        "small_valid_dataset = Subset(valid_dataset, random.sample(range(len(valid_dataset)), 1000))\n",
        "\n",
        "# Create DataLoaders for the smaller datasets\n",
        "small_train_loader = DataLoader(small_train_dataset, batch_size=32, shuffle=True)\n",
        "small_valid_loader = DataLoader(small_valid_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TcwdDOQu3hAR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcwdDOQu3hAR",
        "outputId": "1cf18b8e-1ead-46ef-9038-f37bbfef97aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔁 Starting epoch 1\n",
            "📊 Epoch 1/5, Loss: 733.8252, Train Acc: 77.14%, Val Acc: 84.22%\n",
            "🔁 Starting epoch 2\n",
            "📊 Epoch 2/5, Loss: 457.3976, Train Acc: 87.56%, Val Acc: 86.69%\n",
            "🔁 Starting epoch 3\n",
            "📊 Epoch 3/5, Loss: 312.2584, Train Acc: 91.91%, Val Acc: 91.21%\n",
            "🔁 Starting epoch 4\n",
            "📊 Epoch 4/5, Loss: 211.1498, Train Acc: 94.72%, Val Acc: 92.57%\n",
            "🔁 Starting epoch 5\n",
            "📊 Epoch 5/5, Loss: 140.3167, Train Acc: 96.60%, Val Acc: 93.61%\n"
          ]
        }
      ],
      "source": [
        "# Train the model using the `train_model` function.\n",
        "# The model will be trained for 5 epochs.\n",
        "# The device will be automatically set to 'cuda' if a GPU is available, otherwise 'cpu'.\n",
        "train_model(model, train_loader, valid_loader, epochs=5, device='cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Iih6yusixlJ2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iih6yusixlJ2",
        "outputId": "376037f2-ca2a-4e8a-f392-2667a79a99ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class to index mapping: {'fake': 0, 'real': 1}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Setting the root directory where the training images are stored.\n",
        "# This directory should contain subdirectories for each class (e.g., 'fake' and 'real').\n",
        "data_dir = \"data/train\"\n",
        "\n",
        "# Define a composition of image transformations to be applied to each image.\n",
        "# These transformations are applied on-the-fly as the data is loaded.\n",
        "transform = transforms.Compose([\n",
        "    # Resize each image to a fixed size of 128x128 pixels.\n",
        "    # This is necessary to ensure all images have the same dimensions for batching.\n",
        "    transforms.Resize((128, 128)),\n",
        "    # Convert the PIL Image to a PyTorch tensor.\n",
        "    # This changes the image from a (H x W x C) PIL image to a (C x H x W) PyTorch tensor\n",
        "    # and scales the pixel values from the range [0, 255] to [0.0, 1.0].\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize the tensor image with a mean and standard deviation.\n",
        "    # This normalization is applied to each channel of the tensor image.\n",
        "    # The formula is: output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
        "    # Here, with mean=0.5 and std=0.5, the pixel values are normalized to the range [-1, 1].\n",
        "    # This helps the model to converge faster.\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# Load the entire dataset from the specified directory using ImageFolder.\n",
        "# ImageFolder is a generic data loader where the images are arranged in this way:\n",
        "# root/dog/xxx.png\n",
        "# root/dog/xxy.png\n",
        "# ...\n",
        "# root/cat/123.png\n",
        "# root/cat/456.png\n",
        "# It automatically finds the classes based on the subdirectories.\n",
        "full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
        "\n",
        "# Print the mapping of class names to their corresponding integer labels.\n",
        "# For example, it might print: {'fake': 0, 'real': 1}\n",
        "print(\"Class to index mapping:\", full_dataset.class_to_idx)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets.\n",
        "# 80% of the data will be used for training.\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "# 10% of the data will be used for validation.\n",
        "val_size = int(0.1 * len(full_dataset))\n",
        "# The remaining data (10%) will be used for testing.\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "\n",
        "# Use random_split to perform the split.\n",
        "# This function randomly splits a dataset into non-overlapping new datasets of given lengths.\n",
        "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create DataLoaders for each dataset.\n",
        "# DataLoaders provide an iterable over the given dataset, making it easy to loop through batches of data.\n",
        "# `batch_size` defines the number of samples that will be propagated through the network at once.\n",
        "# `shuffle=True` for the training loader means that the data will be shuffled at every epoch to prevent the model from learning the order of the data.\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bf0792c",
      "metadata": {
        "id": "0bf0792c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2054b63-c072-476b-9cf4-8a56a84a17e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 212.5828\n",
            "Epoch 2, Loss: 52.3367\n",
            "Epoch 3, Loss: 34.2484\n",
            "Epoch 4, Loss: 22.6811\n",
            "Epoch 5, Loss: 16.6982\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "\n",
        "# Set the device to 'cuda' if a GPU is available, otherwise 'cpu'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "# Replace the final fully connected layer with a new one for binary classification\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "# Move the model to the specified device\n",
        "model = model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Define the training function\n",
        "def train_model(model, train_loader, val_loader, epochs=5):\n",
        "    \"\"\"\n",
        "    Trains a PyTorch model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to train.\n",
        "        train_loader (torch.utils.data.DataLoader): DataLoader for the training set.\n",
        "        val_loader (torch.utils.data.DataLoader): DataLoader for the validation set.\n",
        "        epochs (int): The number of training epochs.\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        # Set the model to training mode\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        # Loop through the training data\n",
        "        for images, labels in train_loader:\n",
        "            # Move images and labels to the specified device\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            # Calculate the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "            # Update the running loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Print the epoch results\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eQohNrzQBSWg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQohNrzQBSWg",
        "outputId": "2fb5e63a-66f3-4503-9589-1c74ba1888c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Test Accuracy: 98.38%\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, test_loader, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    \"\"\"\n",
        "    Evaluates a PyTorch model on a test set.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to evaluate.\n",
        "        test_loader (torch.utils.data.DataLoader): DataLoader for the test set.\n",
        "        device (str): The device to evaluate on ('cuda' or 'cpu').\n",
        "    \"\"\"\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    # Move the model to the specified device\n",
        "    model.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Disable gradient calculations\n",
        "    with torch.no_grad():\n",
        "        # Loop through the test data\n",
        "        for images, labels in test_loader:\n",
        "            # Move images and labels to the specified device\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            # Get the predicted labels\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            # Update the total and correct counts\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate the accuracy\n",
        "    accuracy = 100 * correct / total\n",
        "    # Print the test accuracy\n",
        "    print(f\"✅ Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Run the test evaluation\n",
        "evaluate_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xp-3u5ENCCv8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "id": "Xp-3u5ENCCv8",
        "outputId": "be39256e-2303-4a02-e38d-311de75f7d82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Accuracy:  98.38%\n",
            "✅ Precision: 97.66%\n",
            "✅ Recall:    99.11%\n",
            "✅ F1-score:  98.38%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.99      0.98      0.98      5043\n",
            "        Real       0.98      0.99      0.98      4967\n",
            "\n",
            "    accuracy                           0.98     10010\n",
            "   macro avg       0.98      0.98      0.98     10010\n",
            "weighted avg       0.98      0.98      0.98     10010\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Switch model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Lists to collect true and predicted labels\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "# Disable gradient calculations\n",
        "with torch.no_grad():\n",
        "    # Loop through the test data\n",
        "    for images, labels in test_loader:\n",
        "        # Move images and labels to the specified device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        # Get the predicted labels\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # Append the true and predicted labels to the lists\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(true_labels, pred_labels)\n",
        "precision = precision_score(true_labels, pred_labels)\n",
        "recall = recall_score(true_labels, pred_labels)\n",
        "f1 = f1_score(true_labels, pred_labels)\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"✅ Accuracy:  {accuracy * 100:.2f}%\")\n",
        "print(f\"✅ Precision: {precision * 100:.2f}%\")\n",
        "print(f\"✅ Recall:    {recall * 100:.2f}%\")\n",
        "print(f\"✅ F1-score:  {f1 * 100:.2f}%\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(true_labels, pred_labels, target_names=['Fake', 'Real']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from torch import nn\n",
        "\n",
        "# Load the ResNet-18 model architecture without pre-trained weights\n",
        "model = models.resnet18(weights=None)\n",
        "# Replace the final fully connected layer with a new one for binary classification\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "\n",
        "# Define the path to the saved model\n",
        "model_path = \"/content/drive/MyDrive/face-classification-data/models/face_classifier_20250729_194456.pth\"\n",
        "# Load the saved model's state dictionary\n",
        "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
        "# Set the model to evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvwU4uF3wRM7",
        "outputId": "64d9fb0c-70b0-4d05-b0ae-7b10421e1813"
      },
      "id": "OvwU4uF3wRM7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zrg1ZOvdGe7M",
      "metadata": {
        "id": "zrg1ZOvdGe7M"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, UploadFile, File\n",
        "from typing import List\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Define the path to the saved model\n",
        "model_path = \"/content/drive/MyDrive/face-classification-data/models/face_classifier_20250729_194456.pth\"\n",
        "\n",
        "# Load model architecture\n",
        "model = models.resnet18(weights=None)\n",
        "# Replace the final fully connected layer with a new one for binary classification\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "\n",
        "# Load weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "#  Define transforms (should be the same as during training)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5]) # Important to match the training normalization\n",
        "])\n",
        "\n",
        "#  Prediction logic\n",
        "def predict_images(model, images):\n",
        "    \"\"\"\n",
        "    Predicts whether a list of images are real or fake.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The trained model.\n",
        "        images (list): A list of images as bytes.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of predictions, where each prediction is either 'fake' or 'real'.\n",
        "    \"\"\"\n",
        "    # Transform the images and stack them into a batch\n",
        "    batch = torch.stack([\n",
        "        transform(Image.open(io.BytesIO(img)).convert('RGB')) for img in images\n",
        "    ])\n",
        "    # Disable gradient calculations\n",
        "    with torch.no_grad():\n",
        "        # Forward pass\n",
        "        outputs = model(batch.to(device))\n",
        "        # Get the predicted labels\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "    # Map predictions to labels\n",
        "    class_names = ['fake', 'real']\n",
        "    return [class_names[p] for p in preds.tolist()]\n",
        "\n",
        "#  FastAPI app\n",
        "app = FastAPI(title=\"Face Frame Classifier\")\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(files: List[UploadFile] = File(...)):\n",
        "    \"\"\"\n",
        "    An endpoint to predict whether a list of uploaded images are real or fake.\n",
        "\n",
        "    Args:\n",
        "        files (List[UploadFile]): A list of uploaded image files.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the predictions.\n",
        "    \"\"\"\n",
        "    # Read the contents of the uploaded files\n",
        "    contents = [await file.read() for file in files]\n",
        "    # Get the predictions\n",
        "    preds = predict_images(model, contents)\n",
        "    # Return the predictions\n",
        "    return {\"predictions\": preds}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nMEmUehZHN9q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMEmUehZHN9q",
        "outputId": "7925e547-1399-4661-c1ab-2693fc442be1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "#Authentication token from ngrok\n",
        "!ngrok config add-authtoken 30Vshk2e4jx68Twxqv1dYa1apm9_3CzTYsgVqDUwiXx7VTE6a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qsXZYS1JhDsy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsXZYS1JhDsy",
        "outputId": "9c27bc16-958f-424f-807c-1d5531253b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install the pyngrok library, which is a Python wrapper for ngrok.\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tcI7HlO_hH0H",
      "metadata": {
        "id": "tcI7HlO_hH0H"
      },
      "outputs": [],
      "source": [
        "# Import the ngrok library\n",
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VwZJCzN6GjxJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwZJCzN6GjxJ",
        "outputId": "843c0c12-5e98-47b0-c1f2-44fa0b86d632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 FastAPI running at: NgrokTunnel: \"https://0e2e5f1100d3.ngrok-free.app\" -> \"http://localhost:8000\"/docs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [606]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "WARNING:pyngrok.process.ngrok:t=2025-07-29T20:06:44+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8000-7f81b111-c9fe-4422-91e9-3f7dd957cecc acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-07-29T20:06:44+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8000-7f81b111-c9fe-4422-91e9-3f7dd957cecc err=\"failed to start tunnel: session closed\"\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [606]\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "# Apply the nest_asyncio patch to allow the asyncio event loop to be nested.\n",
        "# This is necessary to run uvicorn in a Jupyter/Colab environment.\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Open a public URL to the local port 8000 using ngrok\n",
        "public_url = ngrok.connect(8000)\n",
        "# Print the public URL\n",
        "print(f\"🚀 FastAPI running at: {public_url}/docs\")\n",
        "\n",
        "# Start the FastAPI application using uvicorn\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to the Colab environment.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YGUyvyMtW-M",
        "outputId": "16a77477-4eb2-400f-e023-fdb49179ee5a"
      },
      "id": "1YGUyvyMtW-M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to save the model\n",
        "SAVE_PATH = '/content/drive/MyDrive/face-classification-data/models'\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)"
      ],
      "metadata": {
        "id": "Rg0dxTvTtkiv"
      },
      "id": "Rg0dxTvTtkiv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Define save path (e.g., your Google Drive folder)\n",
        "SAVE_PATH = \"/content/drive/MyDrive/face-classification-data/models\"\n",
        "\n",
        "# Generate timestamped filename\n",
        "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "MODEL_FILENAME = f\"face_classifier_{timestamp}.pth\"\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), os.path.join(SAVE_PATH, MODEL_FILENAME))\n",
        "print(f\"✅ Model saved to: {os.path.join(SAVE_PATH, MODEL_FILENAME)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWkN-hLpt2Ku",
        "outputId": "f65d7ebd-1e2d-4433-c44c-e4eb5f9284e5"
      },
      "id": "WWkN-hLpt2Ku",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved to: /content/drive/MyDrive/face-classification-data/models/face_classifier_20250729_194456.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the model path using the variables from the previous cell\n",
        "model_path = os.path.join(SAVE_PATH, MODEL_FILENAME)\n",
        "\n",
        "# Load the saved model's state dictionary\n",
        "checkpoint = torch.load(model_path, map_location=device)\n",
        "# Load the state dictionary into the model\n",
        "missing, unexpected = model.load_state_dict(checkpoint, strict=False)\n",
        "# Print a success message and any missing or unexpected keys\n",
        "print(\"✅ Model loaded.\")\n",
        "print(\"Missing keys:\", missing)\n",
        "print(\"Unexpected keys:\", unexpected)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVEqB2vTuV4A",
        "outputId": "a8ceadc5-bc0f-447d-921a-fadab80e55d9"
      },
      "id": "wVEqB2vTuV4A",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded.\n",
            "Missing keys: []\n",
            "Unexpected keys: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c69b5285"
      },
      "source": [
        "# Real vs. Fake Face Classification with FastAPI\n",
        "\n",
        "This project demonstrates how to build and deploy a deep learning model to classify images of faces as either \"real\" or \"fake\". The model is trained on a dataset of real and synthetically generated faces and then exposed as a web API using FastAPI and ngrok.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Project Overview](#project-overview)\n",
        "- [Getting Started](#getting-started)\n",
        "  - [Prerequisites](#prerequisites)\n",
        "  - [Installation](#installation)\n",
        "  - [Kaggle API Setup](#kaggle-api-setup)\n",
        "- [Dataset](#dataset)\n",
        "- [Model Training](#model-training)\n",
        "  - [Custom CNN](#custom-cnn)\n",
        "  - [ResNet-18 (Transfer Learning)](#resnet-18-transfer-learning)\n",
        "- [Evaluation](#evaluation)\n",
        "- [API Deployment](#api-deployment)\n",
        "- [How to Use the API](#how-to-use-the-api)\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "The main goal of this project is to create a reliable classifier for distinguishing between real human faces and those generated by AI (e.g., StyleGAN). The process involves:\n",
        "\n",
        "1.  **Data Preparation**: Downloading and preparing the \"140k Real and Fake Faces\" dataset from Kaggle.\n",
        "2.  **Model Training**: Training two different models: a custom Convolutional Neural Network (CNN) and a pre-trained ResNet-18 model using transfer learning.\n",
        "3.  **Model Evaluation**: Evaluating the performance of the trained models using various metrics like accuracy, precision, recall, and a confusion matrix.\n",
        "4.  **API Deployment**: Exposing the trained model as a web API using FastAPI, allowing for real-time predictions.\n",
        "5.  **Public Access**: Using ngrok to create a public URL for the API, making it accessible from anywhere.\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "*   Python 3.x\n",
        "*   Jupyter Notebook or Google Colab\n",
        "*   A Kaggle account and API token\n",
        "*   An ngrok account and authentication token\n",
        "\n",
        "### Installation\n",
        "\n",
        "The necessary Python libraries can be installed by running the first cell of the notebook:\n",
        "\n",
        "### ngrok Authentication\n",
        "\n",
        "I have included my own ngrok authentication token in the notebook, so the API can be run without any additional setup."
      ],
      "id": "c69b5285"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}